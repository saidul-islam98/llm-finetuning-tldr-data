# ğŸ§  Instruction-Finetuning Pipeline for Language Models

This repository provides a complete pipeline to process datasets, train instruction-tuned models using full fine-tuning or LoRA-based fine-tuning, and generate responses using the trained models.

---

## ğŸ“ Directory Structure
```
â”œâ”€â”€ data/ # Processed and raw data directory
â”‚ â”œâ”€â”€ new_test.csv
â”‚ â”œâ”€â”€ new_train.zip
â”‚ â”œâ”€â”€ new_valid.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ Process_data.ipynb # Notebook to convert JSONL to CSV format
â”‚
â”œâ”€â”€ scripts/ # Scripts for preprocessing and generation
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ commands.txt # Commands for preprocessing and training
â”‚ â”œâ”€â”€ generate_response.py # Script to generate model responses
â”‚ â””â”€â”€ process_dataset.py # Converts CSV to Huggingface dataset format
â”‚
â”œâ”€â”€ src/ # Training scripts
â”‚ â”œâ”€â”€ sft.py # Full finetuning
â”‚ â””â”€â”€ sft_lora.py # LoRA finetuning
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“Š Data Format

The `csv` files (`new_train.csv`, `new_valid.csv`, `new_test.csv`) contain structured instruction tuning data with the following format:

| system                     | prompt                                      | output                                     |
|---------------------------|---------------------------------------------|--------------------------------------------|
| System message to the AI  | Instruction for the model                   | Ground-truth output                        |
| You are an AI assistantâ€¦  | You will be given a title and a postâ€¦      | I still have contact with an old exâ€™sâ€¦     |

Each row corresponds to a supervised instruction-response pair.

---

## ğŸ“˜ Notebooks

### `notebooks/Process_data.ipynb`

Use this notebook to **convert `.jsonl` files into `.csv` format** that aligns with the system-prompt-output structure required by the training scripts.

---

## ğŸ”§ Scripts

### Dataset Preprocessing

- `scripts/process_dataset.py`: Converts CSV files into Huggingface `datasets` format.
- `scripts/generate_response.py`: Uses trained models to generate responses from prompts.

### Preprocessing Command

```bash
python scripts/process_dataset.py \
--data_dir data \
--train_file new_train.csv \
--validation_file new_valid.csv \
--test_file new_test.csv \
--save_data_dir data/processed_data_hf
```
## ğŸ§  Training
### Full Finetuning

Use src/sft.py for full model finetuning.
```
python src/sft.py \
--data_dir data/processed_data_hf \
--model_path Qwen/Qwen3-0.6B \
--out_dir path_to_output/training_runs
```
## LoRA Finetuning

Use src/sft_lora.py to perform LoRA-based parameter-efficient finetuning. Adjust script arguments based on your model and hardware setup.
ğŸ“‚ Commands Reference

All relevant commands are provided in scripts/commands.txt, including:
- Preprocessing CSV to Huggingface format
- Running full SFT
- Placeholder for LoRA commands (can be added as needed)

## ğŸš€ Getting Started
- Place your original .jsonl files in the desired location.
- Use notebooks/Process_data.ipynb to convert .jsonl to .csv.
- Use process_dataset.py to convert the CSV into Huggingface format.
- Choose your preferred training script: sft.py or sft_lora.py.
- Train your model and use generate_response.py to produce outputs.

## ğŸ“Œ Notes
- Ensure the dataset follows the correct format: system, prompt, output.
- new_train.zip may contain large-scale training data; unzip it before use.
- All paths can be adjusted via command-line arguments.


